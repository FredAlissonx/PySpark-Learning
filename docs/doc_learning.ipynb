{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Doc Learning\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - agg() Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sample = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 1 - simple operations\n",
    "agg_df_1 = agg_sample.agg({\"age\": \"max\"})\n",
    "agg_df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|min(name)|max(age)|\n",
      "+---------+--------+\n",
      "|    Alice|       5|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 2 - aggregating multiple columns\n",
    "agg_df_2 = agg_sample.agg(\n",
    "    {\n",
    "        \"age\": \"max\",\n",
    "        \"name\": \"min\"\n",
    "    }\n",
    ")\n",
    "agg_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|MAX_AGE|NAME_COUNT|\n",
      "+-------+----------+\n",
      "|      5|         2|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exempla 3 - using sql functions for aggregations\n",
    "agg_df_3 = agg_sample.agg(\n",
    "    F.max(\"age\").alias(\"MAX_AGE\"),\n",
    "    F.count(\"name\").alias(\"NAME_COUNT\")\n",
    ").select(\"MAX_AGE\", \"NAME_COUNT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|count(1)|\n",
      "+---+--------+\n",
      "|  2|       1|\n",
      "|  5|       1|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 4 - using with groupBy\n",
    "agg_df_4 = agg_sample.groupBy(F.col(\"age\")).agg(F.count(\"*\"))\n",
    "agg_df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+---------+\n",
      "|min_age|max_age|avg_age|count_age|\n",
      "+-------+-------+-------+---------+\n",
      "|      2|      5|    3.5|        2|\n",
      "+-------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 5 - Aggregating with Multiple Functions on the Same Column\n",
    "agg_df_5 = agg_sample.agg(\n",
    "    F.min(\"age\").alias(\"min_age\"),\n",
    "    F.max(\"age\").alias(\"max_age\"),\n",
    "    F.avg(\"age\").alias(\"avg_age\"),\n",
    "    F.count(\"age\").alias(\"count_age\")\n",
    ")\n",
    "agg_df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "| name|custom_age_mean|\n",
      "+-----+---------------+\n",
      "|Alice|            2.0|\n",
      "|  Bob|            5.0|\n",
      "+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 6 - using custom aggregations\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Define a UDF for custom aggregation\n",
    "@F.udf(T.DoubleType())\n",
    "def custom_agg(values):\n",
    "    return sum(values) / len(values)  # Custom example: mean calculation\n",
    "\n",
    "# Apply custom aggregation\n",
    "agg_df_6 = agg_sample.groupBy(\"name\").agg(\n",
    "    custom_agg(F.collect_list(\"age\")).alias(\"custom_age_mean\")\n",
    ")\n",
    "agg_df_6.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises \"agg\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---+------+----------+\n",
      "| id|     name|department|age|salary|experience|\n",
      "+---+---------+----------+---+------+----------+\n",
      "|  1|    Alice|     Sales| 34| 70000|         5|\n",
      "|  2|      Bob|        HR| 45| 80000|        10|\n",
      "|  3|Catherine|        IT| 29| 90000|         3|\n",
      "|  4|    David|        IT| 39| 85000|         7|\n",
      "|  5|      Eve|     Sales| 41| 75000|         8|\n",
      "|  6|    Frank|        HR| 30| 60000|         2|\n",
      "|  7|    Grace|        IT| 35| 95000|         6|\n",
      "|  8|   Hannah|     Sales| 50| 65000|        12|\n",
      "|  9|      Ivy|        IT| 38| 87000|         9|\n",
      "| 10|     Jack|        HR| 28| 72000|         4|\n",
      "+---+---------+----------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "data = [(1, \"Alice\", \"Sales\", 34, 70000, 5),\n",
    "        (2, \"Bob\", \"HR\", 45, 80000, 10),\n",
    "        (3, \"Catherine\", \"IT\", 29, 90000, 3),\n",
    "        (4, \"David\", \"IT\", 39, 85000, 7),\n",
    "        (5, \"Eve\", \"Sales\", 41, 75000, 8),\n",
    "        (6, \"Frank\", \"HR\", 30, 60000, 2),\n",
    "        (7, \"Grace\", \"IT\", 35, 95000, 6),\n",
    "        (8, \"Hannah\", \"Sales\", 50, 65000, 12),\n",
    "        (9, \"Ivy\", \"IT\", 38, 87000, 9),\n",
    "        (10, \"Jack\", \"HR\", 28, 72000, 4)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"age\", \"salary\", \"experience\"]\n",
    "\n",
    "agg_sample = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the sample data\n",
    "agg_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------+\n",
      "|max_age|avg_salary|total_experience|\n",
      "+-------+----------+----------------+\n",
      "|     50|   77900.0|              66|\n",
      "+-------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 1: Find the maximum age, average salary, and total experience for all employees.\n",
    "q1 = agg_sample.agg(\n",
    "    F.max(\"age\").alias(\"max_age\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.sum(\"experience\").alias(\"total_experience\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------------+----------------+\n",
      "|department|max_age|       avg_salary|total_experience|\n",
      "+----------+-------+-----------------+----------------+\n",
      "|     Sales|     50|          70000.0|              25|\n",
      "|        HR|     45|70666.66666666667|              16|\n",
      "|        IT|     39|          89250.0|              25|\n",
      "+----------+-------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 2: Group the employees by department and find the maximum age, average salary, and total experience for each department.\n",
    "q2 = agg_sample.groupBy(\"department\").agg(\n",
    "    F.max(\"age\").alias(\"max_age\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.sum(\"experience\").alias(\"total_experience\")\n",
    ").select(\n",
    "    \"department\",\n",
    "    \"max_age\",\n",
    "    \"avg_salary\",\n",
    "    \"total_experience\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+\n",
      "|min_age|max_salary|employee_count|\n",
      "+-------+----------+--------------+\n",
      "|     28|     95000|            10|\n",
      "+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 3: Find the minimum age, maximum salary, and count of employees.\n",
    "q3 = agg_sample.agg(\n",
    "    F.min(\"age\").alias(\"min_age\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.count(\"*\").alias(\"employee_count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+--------------+\n",
      "|department|min_age|max_salary|employee_count|\n",
      "+----------+-------+----------+--------------+\n",
      "|     Sales|     34|     75000|             3|\n",
      "|        HR|     28|     80000|             3|\n",
      "|        IT|     29|     95000|             4|\n",
      "+----------+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 4 - Group the employees by department and find the minimum age,\n",
    "# maximum salary, and count of employees for each department.\n",
    "q4 = agg_sample.groupBy(\"department\").agg(\n",
    "    F.min(\"age\").alias(\"min_age\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.count(\"*\").alias(\"employee_count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|sum(salary)|avg(age)|\n",
      "+-----------+--------+\n",
      "|     779000|    36.9|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 5 - Find the average age and total salary of employees.\n",
    "q5 = agg_sample.agg({\n",
    "    \"age\": \"avg\",\n",
    "    \"salary\": \"sum\"\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|   average_salary|\n",
      "+----------+-----------------+\n",
      "|     Sales|          70000.0|\n",
      "|        HR|70666.66666666667|\n",
      "|        IT|          89250.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 -  Calculate the average salary for each department.\n",
    "q6 = agg_sample.groupBy(\"department\").agg(\n",
    "    F.avg(F.col(\"salary\")).alias(\"average_salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|department|count_employees|\n",
      "+----------+---------------+\n",
      "|     Sales|              3|\n",
      "|        HR|              3|\n",
      "|        IT|              4|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 - Find the total number of employees in each department.\n",
    "q7 = agg_sample.groupBy(\"department\").agg(\n",
    "    F.count(F.col(\"*\")).alias(\"count_employees\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   average_salary|\n",
      "+-----------------+\n",
      "|81166.66666666667|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 - Find the average salary for employees with more than 5 years of experience.\n",
    "q8 = agg_sample.agg(\n",
    "    F.avg(\n",
    "        F.when(F.col(\"experience\") > 5,\n",
    "        F.col(\"salary\"))\n",
    "        ).alias(\"average_salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|department|max_experience|min_experience|\n",
      "+----------+--------------+--------------+\n",
      "|     Sales|            12|             5|\n",
      "+----------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 - Find the maximum and minimum experience for the Sales department.\n",
    "q9 = agg_sample.groupBy(\"department\").agg(\n",
    "    F.max(F.col(\"experience\")).alias(\"max_experience\"),\n",
    "    F.min(F.col(\"experience\")).alias(\"min_experience\")\n",
    ").where(F.col(\"department\") == \"Sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - withColumn function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função withColumn no PySpark é usada para adicionar uma nova coluna a um DataFrame ou para substituir uma coluna existente com base em uma expressão especificada\n",
    "\n",
    "Syntax:\n",
    "\n",
    "**DataFrame.withColumn(colName, col)**\n",
    "\n",
    "colName: O nome da nova coluna ou da coluna existente a ser substituída.\n",
    "col: Uma expressão que define os valores da coluna, que pode ser uma instância de Column, uma expressão SQL, ou uma função do módulo pyspark.sql.functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adicionar uma nova coluna**\n",
    "\n",
    "Vamos adicionar uma nova coluna chamada \"idade_5_anos\" que será a idade atual acrescida de 5 anos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Substituir uma coluna existente**\n",
    "\n",
    "Vamos substituir a coluna \"Idade\" com a idade acrescida de 10 anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"Alice\", \"Sales\", 34, 70000, 5),\n",
    "        (2, \"Bob\", \"HR\", 45, 80000, 10),\n",
    "        (3, \"Catherine\", \"IT\", 29, 90000, 3),\n",
    "        (4, \"David\", \"IT\", 39, 85000, 7),\n",
    "        (5, \"Eve\", \"Sales\", 41, 75000, 8),\n",
    "        (6, \"Frank\", \"HR\", 30, 60000, 2),\n",
    "        (7, \"Grace\", \"IT\", 35, 95000, 6),\n",
    "        (8, \"Hannah\", \"Sales\", 50, 65000, 12),\n",
    "        (9, \"Ivy\", \"IT\", 38, 87000, 9),\n",
    "        (10, \"Jack\", \"HR\", 28, 72000, 4)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"age\", \"salary\", \"experience\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---+------+----------+\n",
      "| id|     name|department|age|salary|experience|\n",
      "+---+---------+----------+---+------+----------+\n",
      "|  1|    Alice|     Sales| 44| 70000|         5|\n",
      "|  2|      Bob|        HR| 55| 80000|        10|\n",
      "|  3|Catherine|        IT| 39| 90000|         3|\n",
      "|  4|    David|        IT| 49| 85000|         7|\n",
      "|  5|      Eve|     Sales| 51| 75000|         8|\n",
      "|  6|    Frank|        HR| 40| 60000|         2|\n",
      "|  7|    Grace|        IT| 45| 95000|         6|\n",
      "|  8|   Hannah|     Sales| 60| 65000|        12|\n",
      "|  9|      Ivy|        IT| 48| 87000|         9|\n",
      "| 10|     Jack|        HR| 38| 72000|         4|\n",
      "+---+---------+----------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"age\", F.col(\"age\") + 10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converter tipos de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---+------+----------+\n",
      "| id|     name|department|age|salary|experience|\n",
      "+---+---------+----------+---+------+----------+\n",
      "|  1|    Alice|     Sales| 44| 70000|         5|\n",
      "|  2|      Bob|        HR| 55| 80000|        10|\n",
      "|  3|Catherine|        IT| 39| 90000|         3|\n",
      "|  4|    David|        IT| 49| 85000|         7|\n",
      "|  5|      Eve|     Sales| 51| 75000|         8|\n",
      "|  6|    Frank|        HR| 40| 60000|         2|\n",
      "|  7|    Grace|        IT| 45| 95000|         6|\n",
      "|  8|   Hannah|     Sales| 60| 65000|        12|\n",
      "|  9|      Ivy|        IT| 48| 87000|         9|\n",
      "| 10|     Jack|        HR| 38| 72000|         4|\n",
      "+---+---------+----------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converter uma coluna de string para inteiro\n",
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"int\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicar funções SQL integradas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---+------+----------+--------+\n",
      "| id|     name|department|age|salary|experience|AnoAtual|\n",
      "+---+---------+----------+---+------+----------+--------+\n",
      "|  1|    Alice|     Sales| 44| 70000|         5|    2024|\n",
      "|  2|      Bob|        HR| 55| 80000|        10|    2024|\n",
      "|  3|Catherine|        IT| 39| 90000|         3|    2024|\n",
      "|  4|    David|        IT| 49| 85000|         7|    2024|\n",
      "|  5|      Eve|     Sales| 51| 75000|         8|    2024|\n",
      "|  6|    Frank|        HR| 40| 60000|         2|    2024|\n",
      "|  7|    Grace|        IT| 45| 95000|         6|    2024|\n",
      "|  8|   Hannah|     Sales| 60| 65000|        12|    2024|\n",
      "|  9|      Ivy|        IT| 48| 87000|         9|    2024|\n",
      "| 10|     Jack|        HR| 38| 72000|         4|    2024|\n",
      "+---+---------+----------+---+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adicionar uma nova coluna \"AnoAtual\" usando a função current_year\n",
    "df = df.withColumn(\"AnoAtual\", F.year(F.current_date()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criar uma coluna calculada com base em outras colunas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `SalarioMensal` cannot be resolved. Did you mean one of the following? [`department`, `experience`, `salary`, `AnoAtual`, `age`].;\n'Project [id#492L, name#493, department#494, age#536, salary#496L, experience#497L, AnoAtual#568, ('SalarioMensal * 12) AS SalarioAnual#605]\n+- Project [id#492L, name#493, department#494, age#536, salary#496L, experience#497L, year(current_date(Some(America/Sao_Paulo))) AS AnoAtual#568]\n   +- Project [id#492L, name#493, department#494, cast(age#504L as int) AS age#536, salary#496L, experience#497L]\n      +- Project [id#492L, name#493, department#494, (age#495L + cast(10 as bigint)) AS age#504L, salary#496L, experience#497L]\n         +- LogicalRDD [id#492L, name#493, department#494, age#495L, salary#496L, experience#497L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Adicionar uma nova coluna \"SalarioAnual\" que é \"SalarioMensal\" vezes 12\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalarioAnual\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalarioMensal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Education\\PySpark-Learning\\.conda\\lib\\site-packages\\pyspark\\sql\\dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5173\u001b[0m     )\n\u001b[1;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Education\\PySpark-Learning\\.conda\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Education\\PySpark-Learning\\.conda\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `SalarioMensal` cannot be resolved. Did you mean one of the following? [`department`, `experience`, `salary`, `AnoAtual`, `age`].;\n'Project [id#492L, name#493, department#494, age#536, salary#496L, experience#497L, AnoAtual#568, ('SalarioMensal * 12) AS SalarioAnual#605]\n+- Project [id#492L, name#493, department#494, age#536, salary#496L, experience#497L, year(current_date(Some(America/Sao_Paulo))) AS AnoAtual#568]\n   +- Project [id#492L, name#493, department#494, cast(age#504L as int) AS age#536, salary#496L, experience#497L]\n      +- Project [id#492L, name#493, department#494, (age#495L + cast(10 as bigint)) AS age#504L, salary#496L, experience#497L]\n         +- LogicalRDD [id#492L, name#493, department#494, age#495L, salary#496L, experience#497L], false\n"
     ]
    }
   ],
   "source": [
    "# Adicionar uma nova coluna \"SalarioAnual\" que é \"SalarioMensal\" vezes 12\n",
    "df = df.withColumn(\"SalarioAnual\", F.col(\"SalarioMensal\") * 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data = [\n",
    "    (\"John\", \"Doe\", 28),\n",
    "    (\"Jane\", \"Smith\", 32),\n",
    "    (\"Mike\", \"Johnson\", 25),\n",
    "    (\"Emily\", \"Brown\", 34),\n",
    "    (\"Kevin\", \"Davis\", 30)\n",
    "]\n",
    "\n",
    "columns = [\"first_name\", \"last_name\", \"age\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+------------+\n",
      "|first_name|last_name|age|   full_name|\n",
      "+----------+---------+---+------------+\n",
      "|      John|      Doe| 28|    John Doe|\n",
      "|      Jane|    Smith| 32|  Jane Smith|\n",
      "|      Mike|  Johnson| 25|Mike Johnson|\n",
      "|     Emily|    Brown| 34| Emily Brown|\n",
      "|     Kevin|    Davis| 30| Kevin Davis|\n",
      "+----------+---------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - Add a new column full_name by concatenating first_name and last_name with a space in between.\n",
    "full_name_df = df.withColumn(\n",
    "    \"full_name\",\n",
    "    F.concat(F.col(\"first_name\"), F.lit(\" \"), F.col(\"last_name\"))\n",
    "    )\n",
    "full_name_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+-----------------+\n",
      "|first_name|last_name|age|age_after_5_years|\n",
      "+----------+---------+---+-----------------+\n",
      "|      John|      Doe| 28|               33|\n",
      "|      Jane|    Smith| 32|               37|\n",
      "|      Mike|  Johnson| 25|               30|\n",
      "|     Emily|    Brown| 34|               39|\n",
      "|     Kevin|    Davis| 30|               35|\n",
      "+----------+---------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - Create a new column age_after_5_years that shows each person's age 5 years from now.\n",
    "age_df = df.withColumn(\n",
    "    \"age_after_5_years\",\n",
    "    F.col(\"age\") + 5\n",
    ")\n",
    "\n",
    "age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+-----------+\n",
      "|first_name|last_name|age|name_length|\n",
      "+----------+---------+---+-----------+\n",
      "|      John|      Doe| 28|          4|\n",
      "|      Jane|    Smith| 32|          4|\n",
      "|      Mike|  Johnson| 25|          4|\n",
      "|     Emily|    Brown| 34|          5|\n",
      "|     Kevin|    Davis| 30|          5|\n",
      "+----------+---------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Add a column name_length that calculates the length of the first_name.\n",
    "name_df = df.withColumn(\n",
    "    \"name_length\",\n",
    "    F.length(\"first_name\")\n",
    ")\n",
    "name_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+-------+\n",
      "|first_name|last_name|age|age_str|\n",
      "+----------+---------+---+-------+\n",
      "|      John|      Doe| 28|     28|\n",
      "|      Jane|    Smith| 32|     32|\n",
      "|      Mike|  Johnson| 25|     25|\n",
      "|     Emily|    Brown| 34|     34|\n",
      "|     Kevin|    Davis| 30|     30|\n",
      "+----------+---------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - Convert the age column to a string type and store it in a new column age_str.\n",
    "str_df = df.withColumn(\n",
    "    \"age_str\",\n",
    "    F.col(\"age\").cast(\"int\")\n",
    ")\n",
    "\n",
    "str_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+--------+\n",
      "|first_name|last_name|age|is_adult|\n",
      "+----------+---------+---+--------+\n",
      "|      John|      Doe| 28|    true|\n",
      "|      Jane|    Smith| 32|    true|\n",
      "|      Mike|  Johnson| 25|    true|\n",
      "|     Emily|    Brown| 34|    true|\n",
      "|     Kevin|    Davis| 30|    true|\n",
      "+----------+---------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 - Create a column is_adult that indicates whether a person is an adult (age >= 18).\n",
    "adult_df = df.withColumn(\n",
    "    \"is_adult\",\n",
    "    F.col(\"age\") >= 18\n",
    ")\n",
    "\n",
    "adult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+-------------+\n",
      "|first_name|last_name|age|name_in_upper|\n",
      "+----------+---------+---+-------------+\n",
      "|      John|      Doe| 28|         JOHN|\n",
      "|      Jane|    Smith| 32|         JANE|\n",
      "|      Mike|  Johnson| 25|         MIKE|\n",
      "|     Emily|    Brown| 34|        EMILY|\n",
      "|     Kevin|    Davis| 30|        KEVIN|\n",
      "+----------+---------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 - Add a column name_in_uppercase that converts the first_name to uppercase.\n",
    "upper_df = df.withColumn(\n",
    "    \"name_in_upper\",\n",
    "    F.upper(\"first_name\")\n",
    ")\n",
    "\n",
    "upper_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+------------+\n",
      "|first_name|last_name|age|age_category|\n",
      "+----------+---------+---+------------+\n",
      "|      John|      Doe| 28|       Young|\n",
      "|      Jane|    Smith| 32|         Old|\n",
      "|      Mike|  Johnson| 25|       Young|\n",
      "|     Emily|    Brown| 34|         Old|\n",
      "|     Kevin|    Davis| 30|         Old|\n",
      "+----------+---------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 - Create a column age_category that categorizes age into \"Young\" (age < 30) and \"Old\" (age >= 30).\n",
    "category_df = df.withColumn(\n",
    "    \"age_category\",\n",
    "    F.when(F.col(\"age\") < 30, \"Young\")\n",
    "    .when(F.col(\"age\") >= 30, \"Old\")\n",
    ")\n",
    "\n",
    "category_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+----------------+\n",
      "|first_name|last_name|age|last_name_length|\n",
      "+----------+---------+---+----------------+\n",
      "|      John|      Doe| 28|               3|\n",
      "|      Jane|    Smith| 32|               5|\n",
      "|      Mike|  Johnson| 25|               7|\n",
      "|     Emily|    Brown| 34|               5|\n",
      "|     Kevin|    Davis| 30|               5|\n",
      "+----------+---------+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 - Add a column last_name_length that calculates the length of the last_name.\n",
    "last_name_df = df.withColumn(\n",
    "    \"last_name_length\",\n",
    "    F.length(\"last_name\")\n",
    ")\n",
    "last_name_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+-----------------+\n",
      "|first_name|last_name|age|name_with_initial|\n",
      "+----------+---------+---+-----------------+\n",
      "|      John|      Doe| 28|           John D|\n",
      "|      Jane|    Smith| 32|           Jane S|\n",
      "|      Mike|  Johnson| 25|           Mike J|\n",
      "|     Emily|    Brown| 34|          Emily B|\n",
      "|     Kevin|    Davis| 30|          Kevin D|\n",
      "+----------+---------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 - Create a column name_with_initial that combines first_name and only the first letter of last_name.\n",
    "initial_df = df.withColumn(\n",
    "    \"name_with_initial\",\n",
    "    F.concat(F.col(\"first_name\"), F.lit(\" \"), F.col(\"last_name\").substr(1, 1))\n",
    ")\n",
    "initial_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+----------+------+---------------+\n",
      "|first_name|last_name|age|department|salary|even_or_odd_age|\n",
      "+----------+---------+---+----------+------+---------------+\n",
      "|      John|      Doe| 28|     Sales|  5000|           Even|\n",
      "|      Jane|    Smith| 32|   Finance|  6000|           Even|\n",
      "|      Mike|  Johnson| 25| Marketing|  4500|            Odd|\n",
      "|     Emily|    Brown| 34|     Sales|  5200|           Even|\n",
      "|     Kevin|    Davis| 30|   Finance|  5800|           Even|\n",
      "|     Laura|   Wilson| 27| Marketing|  4800|            Odd|\n",
      "+----------+---------+---+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 - Add a column even_or_odd_age that labels each person's age as \"Even\" or \"Odd\".\n",
    "even_odd = df.withColumn(\n",
    "    \"even_or_odd_age\",\n",
    "    F.when(F.col(\"age\") % 2 == 0, \"Even\")     \n",
    "    .when(F.col(\"age\") % 2 != 0, \"Odd\")\n",
    ")\n",
    "\n",
    "even_odd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+----------+------+---------+\n",
      "|first_name|last_name|age|department|salary|seniority|\n",
      "+----------+---------+---+----------+------+---------+\n",
      "|      John|      Doe| 28|     Sales|  5000|   Junior|\n",
      "|      Jane|    Smith| 32|   Finance|  6000|   Senior|\n",
      "|      Mike|  Johnson| 25| Marketing|  4500|   Junior|\n",
      "|     Emily|    Brown| 34|     Sales|  5200|   Senior|\n",
      "|     Kevin|    Davis| 30|   Finance|  5800|   Senior|\n",
      "|     Laura|   Wilson| 27| Marketing|  4800|   Junior|\n",
      "+----------+---------+---+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11 - Create a column seniority that labels employees as \"Junior\" (age < 30) and \"Senior\" (age >= 30).\n",
    "level = df.withColumn(\n",
    "    \"seniority\",\n",
    "    F.when(\n",
    "        F.col(\"age\") < 30,\n",
    "        \"Junior\"\n",
    "    )\n",
    "    .when(\n",
    "        F.col(\"age\") >= 30,\n",
    "        \"Senior\"\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+----------+------+------------+\n",
      "|first_name|last_name|age|department|salary|age_category|\n",
      "+----------+---------+---+----------+------+------------+\n",
      "|      John|      Doe| 28|     Sales|  5000|       20-29|\n",
      "|      Jane|    Smith| 32|   Finance|  6000|       30-39|\n",
      "|      Mike|  Johnson| 25| Marketing|  4500|       20-29|\n",
      "|     Emily|    Brown| 34|     Sales|  5200|       30-39|\n",
      "|     Kevin|    Davis| 30|   Finance|  5800|       20-29|\n",
      "|     Laura|   Wilson| 27| Marketing|  4800|       20-29|\n",
      "+----------+---------+---+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12 - Add a column age_category that categorizes age into groups: \"20-29\", \"30-39\".\n",
    "category_df = df.withColumn(\n",
    "    \"age_category\",\n",
    "    F.when(\n",
    "        F.col(\"age\").between(20, 30), \"20-29\"\n",
    "    )\n",
    "    .when(\n",
    "        F.col(\"age\").between(30, 40), \"30-39\"\n",
    "    )\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another exercises - 2 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample\n",
    "data = [\n",
    "    (1, \"Alice\", 30, 5000.0),\n",
    "    (2, \"Bob\", 25, 6000.0),\n",
    "    (3, \"Cathy\", 30, 7000.0),\n",
    "    (4, \"David\", 35, 8000.0),\n",
    "    (5, \"Eve\", 40, 9000.0)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-----------+\n",
      "| id| name|age|salary|  age_group|\n",
      "+---+-----+---+------+-----------+\n",
      "|  1|Alice| 30|5000.0|Middle-aged|\n",
      "|  2|  Bob| 25|6000.0|      Young|\n",
      "|  3|Cathy| 30|7000.0|Middle-aged|\n",
      "|  4|David| 35|8000.0|Middle-aged|\n",
      "|  5|  Eve| 40|9000.0|Middle-aged|\n",
      "+---+-----+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column called age_group that categorizes people into \"Young\" (age < 30),\n",
    "# \"Middle-aged\" (30 <= age < 40), and \"Senior\" (age >= 40).\n",
    "df_1 = df.withColumn(\n",
    "    \"age_group\",\n",
    "    F.when(F.col(\"age\") < 30, \"Young\")\n",
    "    .when(F.col(\"age\").between(30, 40), \"Middle-aged\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+---------------+\n",
      "| id| name|age|salary|salary_increase|\n",
      "+---+-----+---+------+---------------+\n",
      "|  1|Alice| 30|5000.0|         5250.0|\n",
      "|  2|  Bob| 25|6000.0|         6600.0|\n",
      "|  3|Cathy| 30|7000.0|         7350.0|\n",
      "|  4|David| 35|8000.0|         8400.0|\n",
      "|  5|  Eve| 40|9000.0|         9450.0|\n",
      "+---+-----+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column called salary_increase that adds 10% to the salary\n",
    "# if the person is younger than 30, and adds 5% if they are 30 or older.\n",
    "df_2 = df.withColumn(\n",
    "    \"salary_increase\",\n",
    "    F.when(F.col(\"age\") < 30, (F.col(\"salary\") + F.col(\"salary\") * 0.1))\n",
    "    .otherwise(F.col(\"salary\") + F.col(\"salary\") * 0.05)\n",
    ")\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Object Row Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Row Objects\n",
    "# Convert each row of the DataFrame into a Row object and print them\n",
    "rows = df.collect()\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n",
      "Bob\n",
      "Catherine\n",
      "David\n",
      "Eve\n",
      "Frank\n",
      "Grace\n",
      "Hannah\n",
      "Ivy\n",
      "Jack\n"
     ]
    }
   ],
   "source": [
    "# Access Individual Columns\n",
    "# From each Row object, extract and print the \"first_name\" and \"last_name\" fields.\n",
    "rows = df.collect()\n",
    "for row in rows:\n",
    "    name: str = row.name\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n"
     ]
    }
   ],
   "source": [
    "# Calculate Age Sum\n",
    "# Calculate the sum of the ages of all individuals using Row objects.\n",
    "rows = df.collect()\n",
    "sum_age = 0\n",
    "for row in rows:\n",
    "    sum_age += row.age\n",
    "print(sum_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, name='Alice', department='Sales', age=44, salary=70000, experience=5), Row(id=2, name='Bob', department='HR', age=55, salary=80000, experience=10), Row(id=3, name='Catherine', department='IT', age=39, salary=90000, experience=3), Row(id=4, name='David', department='IT', age=49, salary=85000, experience=7), Row(id=5, name='Eve', department='Sales', age=51, salary=75000, experience=8), Row(id=6, name='Frank', department='HR', age=40, salary=60000, experience=2), Row(id=7, name='Grace', department='IT', age=45, salary=95000, experience=6), Row(id=8, name='Hannah', department='Sales', age=60, salary=65000, experience=12), Row(id=9, name='Ivy', department='IT', age=48, salary=87000, experience=9), Row(id=10, name='Jack', department='HR', age=38, salary=72000, experience=4)]\n"
     ]
    }
   ],
   "source": [
    "# Filter by Age\n",
    "# Filter the Row objects to include only those individuals whose age is greater than 30.\n",
    "rows = df.collect()\n",
    "\n",
    "rows = [row for row in rows if row.age > 30]\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---+-----+---+\n",
      "| _1|   _2| _3| _4|   _5| _6|\n",
      "+---+-----+---+---+-----+---+\n",
      "|  4|David| IT| 49|85000|  7|\n",
      "+---+-----+---+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create New DataFrame\n",
    "# Create a new DataFrame using Row objects, but include only individuals whose name starts with the letter \"D\".\n",
    "rows = df.collect()\n",
    "\n",
    "# Filter the rows where name starts with \"D\"\n",
    "filtered_rows = [Row(*row) for row in rows if row.name.startswith(\"D\")]\n",
    "\n",
    "# Create a new DataFrame from the filtered rows\n",
    "new_df = spark.createDataFrame(filtered_rows)\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age normal: 44\n",
      "Age added: 49\n",
      "Age normal: 55\n",
      "Age added: 60\n",
      "Age normal: 39\n",
      "Age added: 44\n",
      "Age normal: 49\n",
      "Age added: 54\n",
      "Age normal: 51\n",
      "Age added: 56\n",
      "Age normal: 40\n",
      "Age added: 45\n",
      "Age normal: 45\n",
      "Age added: 50\n",
      "Age normal: 60\n",
      "Age added: 65\n",
      "Age normal: 48\n",
      "Age added: 53\n",
      "Age normal: 38\n",
      "Age added: 43\n"
     ]
    }
   ],
   "source": [
    "# Modify Age Field\n",
    "# Increase the age of each individual by 5 years using the Row objects.\n",
    "rows = df.collect()\n",
    "\n",
    "for row in rows:\n",
    "    age_normal: int = row.age\n",
    "    age_added: int = row.age + 5\n",
    "    print(\"Age normal:\", age_normal)\n",
    "    print(\"Age added:\", age_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=1, name='Alice', department='Sales', age=44, salary=70000, experience=5)\n",
      "Row(id=2, name='Bob', department='HR', age=55, salary=80000, experience=10)\n",
      "Row(id=3, name='Catherine', department='IT', age=39, salary=90000, experience=3)\n",
      "Row(id=4, name='David', department='IT', age=49, salary=85000, experience=7)\n",
      "Row(id=5, name='Eve', department='Sales', age=51, salary=75000, experience=8)\n",
      "Row(id=6, name='Frank', department='HR', age=40, salary=60000, experience=2)\n",
      "Row(id=7, name='Grace', department='IT', age=45, salary=95000, experience=6)\n",
      "Row(id=8, name='Hannah', department='Sales', age=60, salary=65000, experience=12)\n",
      "Row(id=9, name='Ivy', department='IT', age=48, salary=87000, experience=9)\n",
      "Row(id=10, name='Jack', department='HR', age=38, salary=72000, experience=4)\n"
     ]
    }
   ],
   "source": [
    "# Sort by First Name\n",
    "# Sort the Row objects in ascending order by the \"first_name\" field.\n",
    "rows = df.collect()\n",
    "\n",
    "# Sort the rows by the \"first_name\" field\n",
    "sorted_rows = sorted(rows, key=lambda row: row.name)\n",
    "\n",
    "# Print the sorted rows\n",
    "for row in sorted_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Alice', 'age': 44}, {'name': 'Bob', 'age': 55}, {'name': 'Catherine', 'age': 39}, {'name': 'David', 'age': 49}, {'name': 'Eve', 'age': 51}, {'name': 'Frank', 'age': 40}, {'name': 'Grace', 'age': 45}, {'name': 'Hannah', 'age': 60}, {'name': 'Ivy', 'age': 48}, {'name': 'Jack', 'age': 38}]\n"
     ]
    }
   ],
   "source": [
    "# Custom Field Extraction\n",
    "# Extract the \"name\" and \"age\" fields from each Row object and create a new list of dictionaries with the extracted data.\n",
    "rows = df.collect()\n",
    "\n",
    "# Extract \"name\" and \"age\" fields and create a new list of dictionaries\n",
    "list_of_dicts = [{\"name\": row.name, \"age\": row.age} for row in rows]\n",
    "\n",
    "# Print the list of dictionaries\n",
    "print(list_of_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - drop() Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"grade\", StringType(), True),\n",
    "    StructField(\"details\", StructType([\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "data = [\n",
    "    (1, \"Alice\", 23, \"F\", 50000.0, 101, 250.0, \"alice@example.com\", \"123-456-7890\", \"active\", \"2024-01-01\", \"A\", {\"first_name\": \"Alice\", \"last_name\": \"Smith\", \"age\": 23}),\n",
    "    (2, \"Bob\", 19, \"M\", 60000.0, 102, 150.0, \"bob@example.com\", \"234-567-8901\", \"inactive\", \"2024-02-01\", \"B\", {\"first_name\": \"Bob\", \"last_name\": \"Jones\", \"age\": 19}),\n",
    "    (3, \"Cathy\", 22, \"F\", 70000.0, 103, 300.0, \"cathy@example.com\", \"345-678-9012\", \"active\", \"2024-03-01\", \"C\", {\"first_name\": \"Cathy\", \"last_name\": \"Johnson\", \"age\": 22}),\n",
    "    (4, \"David\", 25, \"M\", 80000.0, 104, 200.0, \"david@example.com\", \"456-789-0123\", \"inactive\", \"2024-04-01\", \"D\", {\"first_name\": \"David\", \"last_name\": \"Williams\", \"age\": 25}),\n",
    "    (5, \"Eve\", 18, \"F\", 90000.0, 105, 350.0, \"eve@example.com\", \"567-890-1234\", \"active\", \"2024-05-01\", \"A\", {\"first_name\": \"Eve\", \"last_name\": \"Brown\", \"age\": 18})\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "| id| name|age|gender| salary|transaction_id|amount|            email|phone_number|  status|      date|grade|             details|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "|  1|Alice| 23|     F|50000.0|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|    A|  {Alice, Smith, 23}|\n",
      "|  2|  Bob| 19|     M|60000.0|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    B|    {Bob, Jones, 19}|\n",
      "|  3|Cathy| 22|     F|70000.0|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|    C|{Cathy, Johnson, 22}|\n",
      "|  4|David| 25|     M|80000.0|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|    D|{David, Williams,...|\n",
      "|  5|  Eve| 18|     F|90000.0|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    A|    {Eve, Brown, 18}|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "| id| name|gender| salary|transaction_id|amount|            email|phone_number|  status|      date|grade|             details|\n",
      "+---+-----+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "|  1|Alice|     F|50000.0|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|    A|  {Alice, Smith, 23}|\n",
      "|  2|  Bob|     M|60000.0|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    B|    {Bob, Jones, 19}|\n",
      "|  3|Cathy|     F|70000.0|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|    C|{Cathy, Johnson, 22}|\n",
      "|  4|David|     M|80000.0|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|    D|{David, Williams,...|\n",
      "|  5|  Eve|     F|90000.0|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    A|    {Eve, Brown, 18}|\n",
      "+---+-----+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Given a DataFrame with columns [\"name\", \"age\", \"gender\"], drop the column age.\n",
    "df_1 = df.drop(\"age\")\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "| id|age|gender|transaction_id|amount|            email|phone_number|  status|      date|grade|             details|\n",
      "+---+---+------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "|  1| 23|     F|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|    A|  {Alice, Smith, 23}|\n",
      "|  2| 19|     M|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    B|    {Bob, Jones, 19}|\n",
      "|  3| 22|     F|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|    C|{Cathy, Johnson, 22}|\n",
      "|  4| 25|     M|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|    D|{David, Williams,...|\n",
      "|  5| 18|     F|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    A|    {Eve, Brown, 18}|\n",
      "+---+---+------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Given a DataFrame with columns [\"id\", \"name\", \"age\", \"salary\"], drop the columns name and salary.\n",
    "df_2 = df.drop(\"name\", \"salary\")\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "| id| name|age|gender| salary|transaction_id|amount|            email|phone_number|  status|      date|grade|             details|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "|  1|Alice| 23|     F|50000.0|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|    A|  {Alice, Smith, 23}|\n",
      "|  2|  Bob| 19|     M|60000.0|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    B|    {Bob, Jones, 19}|\n",
      "|  3|Cathy| 22|     F|70000.0|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|    C|{Cathy, Johnson, 22}|\n",
      "|  4|David| 25|     M|80000.0|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|    D|{David, Williams,...|\n",
      "|  5|  Eve| 18|     F|90000.0|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    A|    {Eve, Brown, 18}|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Given a DataFrame with columns [\"product_id\", \"price\", \"quantity\"], drop the column quantity using a Column object.\n",
    "columns = [column for column in df.columns if column != \"quantity\"]\n",
    "df_3 = df.select(*columns)\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "| id| name|age|gender| salary|transaction_id|amount|            email|phone_number|  status|      date|grade|             details|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "|  1|Alice| 23|     F|50000.0|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|    A|  {Alice, Smith, 23}|\n",
      "|  2|  Bob| 19|     M|60000.0|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    B|    {Bob, Jones, 19}|\n",
      "|  3|Cathy| 22|     F|70000.0|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|    C|{Cathy, Johnson, 22}|\n",
      "|  4|David| 25|     M|80000.0|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|    D|{David, Williams,...|\n",
      "|  5|  Eve| 18|     F|90000.0|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    A|    {Eve, Brown, 18}|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Given a DataFrame with columns [\"id\", \"status\", \"date\"],\n",
    "# attempt to drop a column named address that does not exist in the DataFrame.\n",
    "df_4 = df.drop(\"address\")\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "| id| name|age|gender| salary|transaction_id|amount|            email|phone_number|  status|      date|grade|             details|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "|  1|Alice| 23|     F|50000.0|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|    A|  {Alice, Smith, 23}|\n",
      "|  2|  Bob| 19|     M|60000.0|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    B|    {Bob, Jones, 19}|\n",
      "|  3|Cathy| 22|     F|70000.0|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|    C|{Cathy, Johnson, 22}|\n",
      "|  4|David| 25|     M|80000.0|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|    D|{David, Williams,...|\n",
      "|  5|  Eve| 18|     F|90000.0|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    A|    {Eve, Brown, 18}|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Given a DataFrame with columns [\"transaction_id\", \"amount\", \"amount\"],\n",
    "# first drop duplicate columns and then drop the column amount.\n",
    "\n",
    "# Remove duplicate columns while preserving order\n",
    "unique_columns = list(dict.fromkeys(df.columns))\n",
    "df_5 = df.select(unique_columns)\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+--------------------+\n",
      "| id| name|age|gender| salary|transaction_id|amount|            email|phone_number|  status|      date|             details|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+--------------------+\n",
      "|  1|Alice| 23|     F|50000.0|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|  {Alice, Smith, 23}|\n",
      "|  2|  Bob| 19|     M|60000.0|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    {Bob, Jones, 19}|\n",
      "|  3|Cathy| 22|     F|70000.0|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|{Cathy, Johnson, 22}|\n",
      "|  4|David| 25|     M|80000.0|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|{David, Williams,...|\n",
      "|  5|  Eve| 18|     F|90000.0|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    {Eve, Brown, 18}|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Given a DataFrame with columns [\"student_id\", \"name\", \"age\", \"grade\"],\n",
    "# drop the grade column if the average age of students is greater than 18.\n",
    "df_6 = df\n",
    "avg_age = df.agg(F.avg(\"age\")).collect()[0][0]\n",
    "\n",
    "if avg_age > 18:\n",
    "    df_6 = df_6.drop(\"grade\")\n",
    "    \n",
    "df_6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+-----------+\n",
      "| id| name|age|gender| salary|transaction_id|amount|            email|phone_number|  status|      date|grade|    details|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+-----------+\n",
      "|  1|Alice| 23|     F|50000.0|           101| 250.0|alice@example.com|123-456-7890|  active|2024-01-01|    A|{Alice, 23}|\n",
      "|  2|  Bob| 19|     M|60000.0|           102| 150.0|  bob@example.com|234-567-8901|inactive|2024-02-01|    B|  {Bob, 19}|\n",
      "|  3|Cathy| 22|     F|70000.0|           103| 300.0|cathy@example.com|345-678-9012|  active|2024-03-01|    C|{Cathy, 22}|\n",
      "|  4|David| 25|     M|80000.0|           104| 200.0|david@example.com|456-789-0123|inactive|2024-04-01|    D|{David, 25}|\n",
      "|  5|  Eve| 18|     F|90000.0|           105| 350.0|  eve@example.com|567-890-1234|  active|2024-05-01|    A|  {Eve, 18}|\n",
      "+---+-----+---+------+-------+--------------+------+-----------------+------------+--------+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Given a DataFrame with a column [\"details\"] where details is a struct\n",
    "# containing fields first_name, last_name, and age, drop the field last_name from the struct.\n",
    "\n",
    "df_7 = df.withColumn(\n",
    "    \"details\",\n",
    "    F.struct(\n",
    "        F.col(\"details.first_name\").alias(\"first_name\"),\n",
    "        F.col(\"details.age\").alias(\"age\")\n",
    "    )\n",
    ")\n",
    "df_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------+-----+-----+------+-----------------+--------------------+----------+------+---+\n",
      "|transaction_id|  status|phone_number| name|grade|gender|            email|             details|      date|amount|age|\n",
      "+--------------+--------+------------+-----+-----+------+-----------------+--------------------+----------+------+---+\n",
      "|           101|  active|123-456-7890|Alice|    A|     F|alice@example.com|  {Alice, Smith, 23}|2024-01-01| 250.0| 23|\n",
      "|           102|inactive|234-567-8901|  Bob|    B|     M|  bob@example.com|    {Bob, Jones, 19}|2024-02-01| 150.0| 19|\n",
      "|           103|  active|345-678-9012|Cathy|    C|     F|cathy@example.com|{Cathy, Johnson, 22}|2024-03-01| 300.0| 22|\n",
      "|           104|inactive|456-789-0123|David|    D|     M|david@example.com|{David, Williams,...|2024-04-01| 200.0| 25|\n",
      "|           105|  active|567-890-1234|  Eve|    A|     F|  eve@example.com|    {Eve, Brown, 18}|2024-05-01| 350.0| 18|\n",
      "+--------------+--------+------------+-----+-----+------+-----------------+--------------------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Given a DataFrame with columns [\"a\", \"b\", \"c\", \"d\", \"e\"],\n",
    "# drop the columns c and e, and then reorder the remaining columns in descending order.\n",
    "col = df.drop(\"id\", \"salary\")\n",
    "columns = col.columns\n",
    "\n",
    "ordered_columns = sorted(columns, reverse=True)\n",
    "df_8 = df.select(ordered_columns)\n",
    "\n",
    "df_8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------------+------+--------------------+\n",
      "| id|age| salary|transaction_id|amount|             details|\n",
      "+---+---+-------+--------------+------+--------------------+\n",
      "|  1| 23|50000.0|           101| 250.0|  {Alice, Smith, 23}|\n",
      "|  2| 19|60000.0|           102| 150.0|    {Bob, Jones, 19}|\n",
      "|  3| 22|70000.0|           103| 300.0|{Cathy, Johnson, 22}|\n",
      "|  4| 25|80000.0|           104| 200.0|{David, Williams,...|\n",
      "|  5| 18|90000.0|           105| 350.0|    {Eve, Brown, 18}|\n",
      "+---+---+-------+--------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 - Given a DataFrame with columns [\"col1\", \"col2\", \"col3\", \"col4\"]\n",
    "# of types Integer, String, Integer, and String respectively, drop all columns of type String.\n",
    "df_9 = df\n",
    "dtype_dict = dict(df.dtypes)\n",
    "for col in df_9.columns:\n",
    "    if dtype_dict[col] == \"string\":\n",
    "        df_9 = df_9.drop(col)\n",
    "df_9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - dropna() Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe \n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"details\", StructType([\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Create data with nulls\n",
    "data = [\n",
    "    (1, \"Alice\", 23, 50000.0, {\"first_name\": \"Alice\", \"last_name\": None, \"age\": 23}),\n",
    "    (2, \"Bob\", None, None, {\"first_name\": \"Bob\", \"last_name\": \"Jones\", \"age\": None}),\n",
    "    (3, None, None, 70000.0, {\"first_name\": None, \"last_name\": \"Smith\", \"age\": 22}),\n",
    "    (4, \"David\", 25, None, {\"first_name\": \"David\", \"last_name\": \"Williams\", \"age\": None}),\n",
    "    (5, \"Eve\", 18, 90000.0, None)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+-----------------+\n",
      "| id| name|age| salary|          details|\n",
      "+---+-----+---+-------+-----------------+\n",
      "|  1|Alice| 23|50000.0|{Alice, NULL, 23}|\n",
      "+---+-----+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - Given a DataFrame, drop all rows that contain any null values.\n",
    "df_1 = df.dropna()\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+--------------------+\n",
      "| id| name|age| salary|             details|\n",
      "+---+-----+---+-------+--------------------+\n",
      "|  1|Alice| 23|50000.0|   {Alice, NULL, 23}|\n",
      "|  4|David| 25|   NULL|{David, Williams,...|\n",
      "|  5|  Eve| 18|90000.0|                NULL|\n",
      "+---+-----+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - Given a DataFrame with columns [\"id\", \"name\", \"age\"], drop rows where the age column has null values.\n",
    "df_2 = df.na.drop(subset=[\"age\"])\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-------+--------------------+\n",
      "| id| name| age| salary|             details|\n",
      "+---+-----+----+-------+--------------------+\n",
      "|  1|Alice|  23|50000.0|   {Alice, NULL, 23}|\n",
      "|  3| NULL|NULL|70000.0|   {NULL, Smith, 22}|\n",
      "|  4|David|  25|   NULL|{David, Williams,...|\n",
      "|  5|  Eve|  18|90000.0|                NULL|\n",
      "+---+-----+----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 -Given a DataFrame with columns [\"id\", \"name\", \"age\", \"salary\"],\n",
    "# drop rows where either the age or salary column contains null values.\n",
    "df_3 = df.dropna(how=\"any\", subset=[\"age\", \"salary\"])\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-------+--------------------+\n",
      "| id| name| age| salary|             details|\n",
      "+---+-----+----+-------+--------------------+\n",
      "|  1|Alice|  23|50000.0|   {Alice, NULL, 23}|\n",
      "|  2|  Bob|NULL|   NULL|  {Bob, Jones, NULL}|\n",
      "|  3| NULL|NULL|70000.0|   {NULL, Smith, 22}|\n",
      "|  4|David|  25|   NULL|{David, Williams,...|\n",
      "|  5|  Eve|  18|90000.0|                NULL|\n",
      "+---+-----+----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - Given a DataFrame with columns [\"id\", \"name\", \"age\", \"salary\"], drop rows where all columns are null.\n",
    "df_4 = df.dropna(how=\"all\")\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+--------------------+\n",
      "| id| name|age| salary|             details|\n",
      "+---+-----+---+-------+--------------------+\n",
      "|  1|Alice| 23|50000.0|   {Alice, NULL, 23}|\n",
      "|  4|David| 25|   NULL|{David, Williams,...|\n",
      "|  5|  Eve| 18|90000.0|                NULL|\n",
      "+---+-----+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 - Given a DataFrame with columns [\"id\", \"name\", \"age\", \"salary\"], drop rows if more than one column has null values.\n",
    "df_5 = df.filter(\n",
    "    F.expr(\"size(filter(array(id, name, age, salary), x -> x is null)) <= 1\")\n",
    ")\n",
    "\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+-----------------+\n",
      "| id| name|age| salary|          details|\n",
      "+---+-----+---+-------+-----------------+\n",
      "|  1|Alice| 23|50000.0|{Alice, NULL, 23}|\n",
      "+---+-----+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 - Given a DataFrame, drop all columns that contain any null values.\n",
    "df_6 = df.dropna(how=\"any\")\n",
    "df_6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id| name|             details|\n",
      "+---+-----+--------------------+\n",
      "|  1|Alice|   {Alice, NULL, 23}|\n",
      "|  2|  Bob|  {Bob, Jones, NULL}|\n",
      "|  3| NULL|   {NULL, Smith, 22}|\n",
      "|  4|David|{David, Williams,...|\n",
      "|  5|  Eve|                NULL|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 - Given a DataFrame with columns [\"id\", \"name\", \"age\", \"salary\"], drop columns where more than one row contains null values.\n",
    "# Count nulls per column\n",
    "null_counts = df.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "\n",
    "# Collect null counts and identify columns with more than one null value\n",
    "null_counts_row = null_counts.collect()[0].asDict()\n",
    "columns_to_drop = [c for c in null_counts_row if null_counts_row[c] > 1]\n",
    "\n",
    "# Drop identified columns\n",
    "df_7 = df.drop(*columns_to_drop)\n",
    "df_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+-------+\n",
      "| id|name|age|salary|details|\n",
      "+---+----+---+------+-------+\n",
      "+---+----+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 - Given a DataFrame with a details column that is a struct containing fields\n",
    "# first_name, last_name, and age, drop rows where any field within the struct is null.\n",
    "df_8 = df.filter(\n",
    "    F.col(\"details.first_name\").isNotNull() &\n",
    "    F.col(\"details.last_name\").isNotNull() &\n",
    "    F.col(\"details.age\").isNotNull()\n",
    ")\n",
    "\n",
    "df_8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+-----------------+\n",
      "| id| name|age| salary|          details|\n",
      "+---+-----+---+-------+-----------------+\n",
      "|  1|Alice| 23|50000.0|{Alice, NULL, 23}|\n",
      "+---+-----+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 - Given a DataFrame with columns [\"id\", \"name\", \"age\", \"salary\"],\n",
    "# first filter rows where age is greater than 20, then drop rows with null values in the remaining rows.\n",
    "df_9 = df.filter(F.col(\"age\") > 20)\n",
    "\n",
    "df_9 = df_9.dropna()\n",
    "df_9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-------+-----------------+\n",
      "| id| name|age| salary|          details|\n",
      "+---+-----+---+-------+-----------------+\n",
      "|  1|Alice| 23|50000.0|{Alice, NULL, 23}|\n",
      "+---+-----+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 - Given a DataFrame with columns [\"id\", \"name\", \"age\"], drop rows with null values and then reset the DataFrame index.\n",
    "df_10 = df.dropna()\n",
    "\n",
    "df_10 = df_10.withColumn(\n",
    "    \"id\",\n",
    "    F.monotonically_increasing_id() + 1\n",
    ")\n",
    "df_10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - dropDuplicates() Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender|     State|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 34|     F|  New York|\n",
      "|    Bob| 45|     M|California|\n",
      "|  Alice| 34|     F|  New York|\n",
      "|Charlie| 30|     M|  New York|\n",
      "|   Dave| 40|     M|     Texas|\n",
      "|    Eve| 28|     F|California|\n",
      "|Charlie| 30|     M|  New York|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice\", 34, \"F\", \"New York\"),\n",
    "    (\"Bob\", 45, \"M\", \"California\"),\n",
    "    (\"Alice\", 34, \"F\", \"New York\"),  # Duplicate\n",
    "    (\"Charlie\", 30, \"M\", \"New York\"),\n",
    "    (\"Dave\", 40, \"M\", \"Texas\"),\n",
    "    (\"Eve\", 28, \"F\", \"California\"),\n",
    "    (\"Charlie\", 30, \"M\", \"New York\"),  # Duplicate\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Gender\", \"State\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender|     State|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 34|     F|  New York|\n",
      "|    Bob| 45|     M|California|\n",
      "|   Dave| 40|     M|     Texas|\n",
      "|Charlie| 30|     M|  New York|\n",
      "|    Eve| 28|     F|California|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - Use dropDuplicates() to remove all duplicate rows based on all columns.\n",
    "df_1 = df.dropDuplicates()\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender|     State|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 34|     F|  New York|\n",
      "|    Bob| 45|     M|California|\n",
      "|Charlie| 30|     M|  New York|\n",
      "|   Dave| 40|     M|     Texas|\n",
      "|    Eve| 28|     F|California|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - Remove duplicate rows based on the \"Name\" column only.\n",
    "df_2 = df.dropDuplicates(subset=[\"Name\"])\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender|     State|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 34|     F|  New York|\n",
      "|    Bob| 45|     M|California|\n",
      "|Charlie| 30|     M|  New York|\n",
      "|   Dave| 40|     M|     Texas|\n",
      "|    Eve| 28|     F|California|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Remove duplicates based on the combination of \"Name\" and \"State\" columns.\n",
    "df_3 = df.dropDuplicates(subset=[\"Name\", \"State\"])\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender|     State|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 34|     F|  New York|\n",
      "|    Bob| 45|     M|California|\n",
      "|   Dave| 40|     M|     Texas|\n",
      "|Charlie| 30|     M|  New York|\n",
      "|    Eve| 28|     F|California|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - Remove duplicates and keep the first occurrence of each row.\n",
    "#  Alice| 34|     F|  New York\n",
    "#  Charlie| 30|     M|  New York\n",
    "df_4 = df.dropDuplicates()\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+\n",
      "|   Name|Age|Gender|     State|\n",
      "+-------+---+------+----------+\n",
      "|  Alice| 34|     F|  New York|\n",
      "|    Bob| 45|     M|California|\n",
      "|   Dave| 40|     M|     Texas|\n",
      "|Charlie| 30|     M|  New York|\n",
      "|    Eve| 28|     F|California|\n",
      "+-------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 - Remove duplicates and keep the last occurrence of each row.\n",
    "df_5 = df.dropDuplicates()\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 - Count the number of rows in the DataFrame after removing duplicates based on all columns.\n",
    "df_6 = df.dropDuplicates()\n",
    "df_6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df 7: 5\n",
      "Original DF: 7\n"
     ]
    }
   ],
   "source": [
    "# 7 - Compare the count of rows before and after using dropDuplicates() on the \"Name\" column.\n",
    "df_7 = df.dropDuplicates(subset=[\"Name\"])\n",
    "print(f\"Df 7: {df_7.count()}\")\n",
    "print(f\"Original DF: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| Name|     State|\n",
      "+-----+----------+\n",
      "|  Bob|California|\n",
      "|Alice|  New York|\n",
      "| Dave|     Texas|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 - Remove duplicates based on the \"State\" column and show only the \"Name\" and \"State\" columns in the result.\n",
    "df_8 = df.dropDuplicates(subset=[\"State\"]).select(\n",
    "    \"Name\",\n",
    "    \"State\"\n",
    ")\n",
    "\n",
    "df_8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not same\n"
     ]
    }
   ],
   "source": [
    "# 9 - Remove duplicates based on \"Age\" and verify if the resulting DataFrame still contains any duplicate rows.\n",
    "df_9 = df.dropDuplicates(subset=[\"Age\"])\n",
    "original_df_count = df.count()\n",
    "df_9_count = df_9.count()\n",
    "\n",
    "if original_df_count == df_9_count:\n",
    "    print(\"Same\")\n",
    "else:\n",
    "    print(\"Not same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+----------+\n",
      "| Name|Age|Gender|     State|\n",
      "+-----+---+------+----------+\n",
      "|  BOB| 45|     M|California|\n",
      "|ALICE| 34|     F|  New York|\n",
      "| DAVE| 40|     M|     Texas|\n",
      "+-----+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 - Remove duplicates based on \"State\" and then apply a transformation (e.g., converting \"Name\" to uppercase)\n",
    "# to the resulting DataFrame.\n",
    "df_10 = df.dropDuplicates(subset=[\"State\"])\n",
    "df_10 = df_10.withColumn(\n",
    "    \"Name\",\n",
    "    F.upper(F.col(\"Name\"))\n",
    ")\n",
    "df_10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - selectExpr() Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+\n",
      "|   Name|Age|Gender|Salary|\n",
      "+-------+---+------+------+\n",
      "|  Alice| 34|     F|  7000|\n",
      "|    Bob| 45|     M|  8000|\n",
      "|Charlie| 30|     M| 12000|\n",
      "|   Dave| 40|     M| 15000|\n",
      "|    Eve| 28|     F|  5000|\n",
      "+-------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SelectExprExample\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", 34, \"F\", 7000),\n",
    "    (\"Bob\", 45, \"M\", 8000),\n",
    "    (\"Charlie\", 30, \"M\", 12000),\n",
    "    (\"Dave\", 40, \"M\", 15000),\n",
    "    (\"Eve\", 28, \"F\", 5000),\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Gender\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   Name|Salary|\n",
      "+-------+------+\n",
      "|  Alice|  7000|\n",
      "|    Bob|  8000|\n",
      "|Charlie| 12000|\n",
      "|   Dave| 15000|\n",
      "|    Eve|  5000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - Use selectExpr() to select only the \"Name\" and \"Salary\" columns.\n",
    "df_1 = df.selectExpr(\"Name\", \"Salary\")\n",
    "df_1.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Income|\n",
      "+------+\n",
      "|  7000|\n",
      "|  8000|\n",
      "| 12000|\n",
      "| 15000|\n",
      "|  5000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - Rename the \"Salary\" column to \"Income\" using selectExpr().\n",
    "df_2 = df.selectExpr(\n",
    "    \"Salary AS Income\"\n",
    ")\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+------+\n",
      "|   Name|Age|Gender|Salary|   Tax|\n",
      "+-------+---+------+------+------+\n",
      "|  Alice| 34|     F|  7000| 700.0|\n",
      "|    Bob| 45|     M|  8000| 800.0|\n",
      "|Charlie| 30|     M| 12000|1200.0|\n",
      "|   Dave| 40|     M| 15000|1500.0|\n",
      "|    Eve| 28|     F|  5000| 500.0|\n",
      "+-------+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Add a new column \"Tax\" which is 10% of \"Salary\" using selectExpr().\n",
    "df_3 = df.selectExpr(\n",
    "    \"*\",\n",
    "    \"Salary * 0.1 AS Tax\"\n",
    ")\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+\n",
      "|   Name|Age|Gender|Salary|\n",
      "+-------+---+------+------+\n",
      "|    Bob| 45|     M|  8000|\n",
      "|Charlie| 30|     M| 12000|\n",
      "|   Dave| 40|     M| 15000|\n",
      "+-------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - Select rows where \"Salary\" is greater than 7000 using selectExpr().\n",
    "df_4 = df.filter(F.col(\"Salary\") > 7000)\n",
    "df_4 = df_4.selectExpr(\"*\")\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+--------------+\n",
      "|   Name|Age|Gender|Salary|AdjustedSalary|\n",
      "+-------+---+------+------+--------------+\n",
      "|  Alice| 34|     F|  7000|          7500|\n",
      "|    Bob| 45|     M|  8000|          8500|\n",
      "|Charlie| 30|     M| 12000|         12500|\n",
      "|   Dave| 40|     M| 15000|         15500|\n",
      "|    Eve| 28|     F|  5000|          5500|\n",
      "+-------+---+------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 - Add a new column \"AdjustedSalary\" which is the \"Salary\" plus 500 using selectExpr().\n",
    "df_5 = df.selectExpr(\n",
    "    \"*\",\n",
    "    \"Salary + 500 AS AdjustedSalary\"\n",
    ")\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|   Name|SalaryWithBonus|\n",
      "+-------+---------------+\n",
      "|  Alice|           8000|\n",
      "|    Bob|           9000|\n",
      "|Charlie|          13000|\n",
      "|   Dave|          16000|\n",
      "|    Eve|           6000|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 - Select the \"Name\" and a calculated \"SalaryWithBonus\" column, where the bonus is 1000 using selectExpr().\n",
    "df_6 = df.selectExpr(\n",
    "    \"Name\",\n",
    "    \"Salary + 1000 AS SalaryWithBonus\"\n",
    ")\n",
    "\n",
    "df_6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+-----------------+\n",
      "|   Name|Age|Gender|Salary|   NameWithPrefix|\n",
      "+-------+---+------+------+-----------------+\n",
      "|  Alice| 34|     F|  7000|  Employee: Alice|\n",
      "|    Bob| 45|     M|  8000|    Employee: Bob|\n",
      "|Charlie| 30|     M| 12000|Employee: Charlie|\n",
      "|   Dave| 40|     M| 15000|   Employee: Dave|\n",
      "|    Eve| 28|     F|  5000|    Employee: Eve|\n",
      "+-------+---+------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 - Create a new column \"NameWithPrefix\" that adds the prefix \"Employee: \" to the \"Name\" column using selectExpr().\n",
    "df_7 = df.selectExpr(\n",
    "    \"*\",\n",
    "    \"CONCAT('Employee: ', Name) AS NameWithPrefix\"\n",
    ")\n",
    "df_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|AverageSalary|\n",
      "+-------------+\n",
      "|       9400.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 - Use selectExpr() to calculate the average salary of all employees.\n",
    "df_8 = df.selectExpr(\n",
    "    \"AVG(salary) AS AverageSalary\"\n",
    ")\n",
    "df_8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+---------+------+\n",
      "|   Name|Age|Gender|Salary|SalaryInk|Senior|\n",
      "+-------+---+------+------+---------+------+\n",
      "|  Alice| 34|     F|  7000|      7.0|    No|\n",
      "|    Bob| 45|     M|  8000|      8.0|   Yes|\n",
      "|Charlie| 30|     M| 12000|     12.0|    No|\n",
      "|   Dave| 40|     M| 15000|     15.0|   Yes|\n",
      "|    Eve| 28|     F|  5000|      5.0|    No|\n",
      "+-------+---+------+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9 - Select the \"Name\" and two new columns:\n",
    "# \"SalaryInK\" (Salary divided by 1000) and \"Senior\" (if Age is greater than 40, 'Yes', otherwise 'No') using selectExpr().\n",
    "df_9 = df.selectExpr(\n",
    "    \"*\",\n",
    "    \"Salary / 1000 AS SalaryInk\",\n",
    "    \"\"\"\n",
    "    CASE\n",
    "        WHEN Age >= 40 THEN 'Yes'\n",
    "        ELSE 'No'\n",
    "    END AS Senior\n",
    "    \"\"\"\n",
    ")\n",
    "df_9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+------------+\n",
      "|   Name|Age|Gender|Salary|Compensation|\n",
      "+-------+---+------+------+------------+\n",
      "|  Alice| 34|     F|  7000|      $8,400|\n",
      "|    Bob| 45|     M|  8000|      $9,600|\n",
      "|Charlie| 30|     M| 12000|     $14,400|\n",
      "|   Dave| 40|     M| 15000|     $18,000|\n",
      "|    Eve| 28|     F|  5000|      $6,000|\n",
      "+-------+---+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 - Create a new column \"Compensation\" which is the product of \"Salary\"\n",
    "# and a factor of 1.2, and format it as currency (e.g., \"$12,000\") using selectExpr().\n",
    "df_10 = df.selectExpr(\n",
    "    \"*\",\n",
    "    \"CONCAT('$', FORMAT_NUMBER(Salary * 1.2, 0)) AS Compensation\"\n",
    ")\n",
    "df_10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - cache() quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: What is the primary purpose of the cache() method in PySpark?\n",
    "The primary purpose of the cache is to store the intermediate results of a DF or RDD in memory (RAM). It helps to optimize the performance or multiple actions on the same DF/RDD by avoiding recomputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: How does the cache() method improve the performance of Spark applications?\n",
    "The cache improve the performance by avoiding recomputations on a DF/OS, one case is when the same DF/RDD is getting multiple actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: What is the difference between cache() and persist() in PySpark?\n",
    "The difference between both are that cache() is a shortcut for persist and you cannot set the StorageLevel, so it work only on the default LevelStorage (MEMORY_AND_DISK_DESER).\n",
    "With persist() you can set the StorageLevel based on what you want, the default StorageLevel also is MEMORY_AND_DISK_DESER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: What is the default storage level used by the cache() method?\n",
    "MEMORY_AND_DISK_DESER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Name at least three different storage levels available in PySpark.\n",
    "1 - MEMORY_ONLY,\n",
    "2 - MEMORY_AND_DISK_SER_2, \n",
    "3 - DISK_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: How can you check if an RDD or DataFrame is cached?\n",
    "df.is_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: How does caching affect the memory usage in a Spark cluster?\n",
    "Caching increases memory usage by storing intermediate results in memory to improve performance. If memory is insufficient, Spark may spill data to disk, and inefficient management of cached data can lead to memory pressure and potential performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: What happens if you call cache() on a DataFrame that is already cached?\n",
    "No, calling cache() on a DataFrame that is already cached will not store the computation again. When you call cache(), Spark marks the DataFrame for caching, but it does not duplicate the cached data or re-compute the DataFrame. The DataFrame will continue to use the already cached data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: Can you use cache() on both DataFrames and RDDs in PySpark?\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: How do you remove a cached DataFrame or RDD from memory?\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - persist() quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - What is the purpose of the persist() method in PySpark?\n",
    "The persist() method in PySpark is used to store (persist) partitions of the DataFrame or RDD in memory and/or disk with a specific storage level. This helps avoid recomputation of the DataFrame or RDD when it is reused in multiple actions or transformations, leading to improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - How does persist() differ from cache() in terms of functionality?\n",
    "While cache() you cannot set storage level, with persist() you can set storage level with a lot of options, but if you do not set storage level, they will have the same functionality, both have storage level default MEMORY_AND_DISK_DESER, so cache() is a shortcut for persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Name the default storage level used by persist() when no level is specified.\n",
    "MEMORY_AND_DISK_DESER (Spark 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - What are the different storage levels available when using persist() in PySpark?\n",
    "1 - StorageLevel.MEMORY_ONLY: Stores DFs/RDDs partitions on memory only \\\n",
    "2 - StorageLevel.MEMORY_ONLY_2: Stores DFs/RDDs partitions on memory replicating each partitions to two clusters nodes \\\n",
    "3 - StorageLevel.MEMORY_AND_DISK: Stores DFs/RDDs partitions on memory, if memory is not enough Disk will store \\\n",
    "4 - StorageLevel.MEMORY_AND_DISK_2: Stores DFs/RDDs partitions on memory, if memory is not enough Disk will store, replicating each partitions to two clusters nodes \\\n",
    "5 - StorageLevel.DISK_ONLY: Stores DFs/RDDs partitions on disk only \\\n",
    "6 - StorageLevel.DISK_ONLY_2: Stores DFs/RDDs partitions on disk only, replicating each partitions to two cluster nodes \\\n",
    "7 - StorageLevel.MEMORY_AND_DISK_DESER: Stores DFs/RDDs as serialized objects in JVM memory and disk is space is not enough. \\\n",
    "8 - StorageLevel.MEMORY_ONLY_DESER: Stores DFs/RDDs as serialized objects in JVM memory. Takes lesser memory but requires additional CPU cycles for deserialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 - Explain the MEMORY_AND_DISK storage level in the context of persist().\n",
    "Stores the DataFrame or RDD partitions in memory. If the memory is insufficient, it spills the excess partitions to disk, ensuring that data is still persisted and accessible without recomputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 - How do you specify a custom storage level when persisting a DataFrame or RDD?\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 - What impact does using the persist() method have on the resources of a Spark cluster?\n",
    "Using persist() increases memory and potentially disk usage depending on the storage level specified. It can also lead to increased network and I/O usage if replication is involved. While persist() can improve performance by avoiding recomputation, it requires careful management of resources to balance memory, disk usage, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 - Why might you choose to use persist() over cache() in certain scenarios?\n",
    "Choosing persist() over cache() is beneficial when you need flexibility in specifying the storage level to suit specific needs or optimize resource usage. persist() allows for tailored data storage strategies, which can improve performance and efficiency based on the characteristics of your data and application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 - How can you verify which storage level is being used by a persisted DataFrame or RDD?\n",
    "df: df.storagelevel \\\n",
    "rdd: rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 - How does the persist() method contribute to fault tolerance in Spark?\n",
    "The persist() method enhances fault tolerance by allowing data to be stored with replication across multiple nodes, ensuring that data is not lost if a node fails. Additionally, Spark’s lineage information provides a fallback mechanism for recomputing lost data, contributing to overall reliability and robustness in the face of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", 34, \"F\", 7000),\n",
    "    (\"Bob\", 45, \"M\", 8000),\n",
    "    (\"Charlie\", 30, \"M\", 12000),\n",
    "    (\"Dave\", 40, \"M\", 15000),\n",
    "    (\"Eve\", 28, \"F\", 5000),\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"gender\", \"salary\"]\n",
    "\n",
    "df_test = spark.createDataFrame(data=data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"salary\"] + [col for col in df_test.columns if col != \"salary\"]\n",
    "df_2 = df_test.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary', 'name', 'age', 'gender']\n"
     ]
    }
   ],
   "source": [
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+------+\n",
      "|   name|age|gender|salary|\n",
      "+-------+---+------+------+\n",
      "|  Alice| 34|     F|  7000|\n",
      "|    Bob| 45|     M|  8000|\n",
      "|Charlie| 30|     M| 12000|\n",
      "|   Dave| 40|     M| 15000|\n",
      "|    Eve| 28|     F|  5000|\n",
      "+-------+---+------+------+\n",
      "\n",
      "+------+-------+---+------+\n",
      "|salary|   name|age|gender|\n",
      "+------+-------+---+------+\n",
      "|  7000|  Alice| 34|     F|\n",
      "|  8000|    Bob| 45|     M|\n",
      "| 12000|Charlie| 30|     M|\n",
      "| 15000|   Dave| 40|     M|\n",
      "|  5000|    Eve| 28|     F|\n",
      "+------+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|transactions|\n",
      "+---+------------+\n",
      "| 44|      972.26|\n",
      "| 35|       530.3|\n",
      "| 63|       93.22|\n",
      "| 58|      289.05|\n",
      "| 71|      867.56|\n",
      "| 18|      999.02|\n",
      "| 71|      875.13|\n",
      "| 21|      550.16|\n",
      "| 14|      146.77|\n",
      "| 41|       637.4|\n",
      "| 46|      282.64|\n",
      "| 78|      970.33|\n",
      "| 21|      284.64|\n",
      "| 88|      710.23|\n",
      "| 82|      485.83|\n",
      "|  1|      769.36|\n",
      "| 47|      431.35|\n",
      "| 27|      760.55|\n",
      "| 78|      170.72|\n",
      "| 12|       986.6|\n",
      "+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, round, count\n",
    "\n",
    "num_rows = 1_000 # quantidade de linhas\n",
    "\n",
    "df_origin = spark.range(0, num_rows) \\\n",
    "    .withColumn(\"id\", (round(rand() * 100)).cast(\"int\")) \\\n",
    "    .withColumn(\"transactions\", round(rand() * 1000, 2))\n",
    "    \n",
    "df_origin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|median_transactions|\n",
      "+---+-------------------+\n",
      "|  0|            307.185|\n",
      "|  1|             506.08|\n",
      "|  2|             304.94|\n",
      "|  3|             283.48|\n",
      "|  4|            190.385|\n",
      "|  5|             623.65|\n",
      "|  6|            486.985|\n",
      "|  7| 450.33500000000004|\n",
      "|  8| 439.34000000000003|\n",
      "|  9|             526.21|\n",
      "+---+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, count, when, lit, avg, row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"transactions\")\n",
    "\n",
    "df_with_row_num = df_origin.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                    .withColumn(\"total_count\", count(\"transactions\").over(Window.partitionBy(\"id\")))\n",
    "\n",
    "df_median = df_with_row_num.filter(\n",
    "    when(\n",
    "        col(\"total_count\") % 2 == 0,\n",
    "        (col(\"row_num\") == col(\"total_count\") / 2) | (col(\"row_num\") == col(\"total_count\") / 2 + 1)\n",
    "    ).otherwise(col(\"row_num\") == col(\"total_count\") / 2 + 0.5)\n",
    ").groupBy(\"id\").agg(avg(\"transactions\").alias(\"median_transactions\"))\n",
    "\n",
    "\n",
    "df_median.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------------------+\n",
      "|id |transactions|median_transactions|\n",
      "+---+------------+-------------------+\n",
      "|0  |471.07      |307.185            |\n",
      "|0  |143.3       |307.185            |\n",
      "|1  |542.17      |506.08             |\n",
      "|1  |967.54      |506.08             |\n",
      "|1  |211.4       |506.08             |\n",
      "|1  |229.46      |506.08             |\n",
      "|1  |367.43      |506.08             |\n",
      "|1  |715.07      |506.08             |\n",
      "|1  |648.83      |506.08             |\n",
      "|1  |216.59      |506.08             |\n",
      "|1  |506.08      |506.08             |\n",
      "|1  |983.58      |506.08             |\n",
      "|1  |255.17      |506.08             |\n",
      "|1  |13.1        |506.08             |\n",
      "|1  |506.56      |506.08             |\n",
      "|1  |29.27       |506.08             |\n",
      "|1  |769.36      |506.08             |\n",
      "|2  |173.74      |304.94             |\n",
      "|2  |578.77      |304.94             |\n",
      "|2  |336.72      |304.94             |\n",
      "|2  |103.76      |304.94             |\n",
      "|2  |318.94      |304.94             |\n",
      "|2  |3.19        |304.94             |\n",
      "|2  |392.05      |304.94             |\n",
      "|2  |290.94      |304.94             |\n",
      "|3  |94.29       |283.48             |\n",
      "|3  |579.83      |283.48             |\n",
      "|3  |155.06      |283.48             |\n",
      "|3  |563.16      |283.48             |\n",
      "|3  |240.99      |283.48             |\n",
      "|3  |749.5       |283.48             |\n",
      "|3  |283.48      |283.48             |\n",
      "|3  |249.59      |283.48             |\n",
      "|3  |796.85      |283.48             |\n",
      "|3  |143.51      |283.48             |\n",
      "|3  |402.98      |283.48             |\n",
      "|4  |464.04      |190.385            |\n",
      "|4  |241.18      |190.385            |\n",
      "|4  |139.59      |190.385            |\n",
      "|4  |609.14      |190.385            |\n",
      "|4  |107.88      |190.385            |\n",
      "|4  |352.43      |190.385            |\n",
      "|4  |492.09      |190.385            |\n",
      "|4  |71.6        |190.385            |\n",
      "|4  |5.48        |190.385            |\n",
      "|4  |33.38       |190.385            |\n",
      "|5  |849.3       |623.65             |\n",
      "|5  |623.65      |623.65             |\n",
      "|5  |165.22      |623.65             |\n",
      "|5  |719.52      |623.65             |\n",
      "|5  |84.22       |623.65             |\n",
      "|5  |830.88      |623.65             |\n",
      "|5  |291.16      |623.65             |\n",
      "|6  |399.24      |486.985            |\n",
      "|6  |504.02      |486.985            |\n",
      "|6  |322.84      |486.985            |\n",
      "|6  |672.97      |486.985            |\n",
      "|6  |957.85      |486.985            |\n",
      "|6  |294.09      |486.985            |\n",
      "|6  |945.78      |486.985            |\n",
      "|6  |885.56      |486.985            |\n",
      "|6  |342.32      |486.985            |\n",
      "|6  |754.98      |486.985            |\n",
      "|6  |469.95      |486.985            |\n",
      "|6  |34.61       |486.985            |\n",
      "|7  |424.01      |450.33500000000004 |\n",
      "|7  |476.66      |450.33500000000004 |\n",
      "|7  |883.99      |450.33500000000004 |\n",
      "|7  |417.97      |450.33500000000004 |\n",
      "|7  |740.38      |450.33500000000004 |\n",
      "|7  |593.84      |450.33500000000004 |\n",
      "|7  |887.72      |450.33500000000004 |\n",
      "|7  |61.05       |450.33500000000004 |\n",
      "|7  |46.67       |450.33500000000004 |\n",
      "|7  |108.41      |450.33500000000004 |\n",
      "|8  |829.58      |439.34000000000003 |\n",
      "|8  |609.97      |439.34000000000003 |\n",
      "|8  |216.15      |439.34000000000003 |\n",
      "|8  |385.67      |439.34000000000003 |\n",
      "|8  |220.44      |439.34000000000003 |\n",
      "|8  |900.52      |439.34000000000003 |\n",
      "|8  |493.01      |439.34000000000003 |\n",
      "|8  |280.74      |439.34000000000003 |\n",
      "|8  |70.18       |439.34000000000003 |\n",
      "|8  |716.09      |439.34000000000003 |\n",
      "|8  |848.86      |439.34000000000003 |\n",
      "|8  |9.13        |439.34000000000003 |\n",
      "|9  |68.83       |526.21             |\n",
      "|9  |677.17      |526.21             |\n",
      "|9  |223.34      |526.21             |\n",
      "|9  |298.18      |526.21             |\n",
      "|9  |718.32      |526.21             |\n",
      "|9  |526.21      |526.21             |\n",
      "|9  |997.24      |526.21             |\n",
      "|10 |937.61      |878.75             |\n",
      "|10 |334.31      |878.75             |\n",
      "|10 |387.01      |878.75             |\n",
      "|10 |878.75      |878.75             |\n",
      "|10 |928.52      |878.75             |\n",
      "|10 |219.43      |878.75             |\n",
      "+---+------------+-------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df_origin.join(df_median, on=\"id\", how=\"inner\") \\\n",
    "    .select(df_origin[\"*\"], df_median[\"median_transactions\"]) \\\n",
    "    .orderBy(\"id\")\n",
    "    \n",
    "result_df.show(100, truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
